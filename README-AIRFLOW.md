# ðŸš€ DGU Futebol - Apache Airflow 3.0

DocumentaÃ§Ã£o completa para execuÃ§Ã£o do pipeline DGU com Apache Airflow 3.0.

## ðŸ“‹ VisÃ£o Geral

O Apache Airflow orquestra automaticamente o pipeline de dados de futebol, executando:

- **ExtraÃ§Ã£o de dados** dos sites fbref.com e transfermarkt.com
- **Processamento via dbt** (staging e mart)
- **Carregamento no BigQuery**
- **Monitoramento e alertas**

### ðŸ• Agendamento AutomÃ¡tico

- **TerÃ§as-feiras Ã s 09:00** - ExecuÃ§Ã£o completa do pipeline
- **Sextas-feiras Ã s 09:00** - ExecuÃ§Ã£o completa do pipeline
- **Diariamente Ã s 10:00** - Monitoramento e verificaÃ§Ãµes de qualidade

## ðŸš€ InÃ­cio RÃ¡pido

### 1. ConfiguraÃ§Ã£o Inicial (5 min)

```bash
# Preparar credenciais
mkdir -p credentials
cp /caminho/para/suas/credenciais.json credentials/bigquery-credentials.json

# Configurar dbt
cp DGU/profiles.yml.example DGU/profiles.yml
# Editar DGU/profiles.yml com suas configuraÃ§Ãµes

# Inicializar Airflow
./docker-scripts.sh init-airflow
```

### 2. Acessar Interface Web

```bash
# Abrir no navegador
http://localhost:8080

# Credenciais padrÃ£o
UsuÃ¡rio: airflow
Senha: airflow
```

### 3. Ativar DAGs

Na interface web do Airflow:
1. Acesse a pÃ¡gina principal
2. Ative as DAGs:
   - `dgu_futebol_pipeline` (Pipeline principal)
   - `dgu_monitoring` (Monitoramento)

## ðŸ“Š DAGs DisponÃ­veis

### ðŸ† `dgu_futebol_pipeline`
**Pipeline principal de dados de futebol**

- **Agendamento**: TerÃ§as e sextas Ã s 09:00
- **DuraÃ§Ã£o**: ~30-45 minutos
- **Tasks**:
  1. ExtraÃ§Ã£o de estatÃ­sticas (fbref.com)
  2. ExtraÃ§Ã£o de valores de mercado (transfermarkt.com)
  3. Processamento e salvamento de seeds
  4. VerificaÃ§Ã£o de dados extraÃ­dos
  5. dbt seed (carregamento no BigQuery)
  6. dbt run staging (views de limpeza)
  7. dbt run mart (tabelas finais)
  8. dbt test (validaÃ§Ãµes)
  9. VerificaÃ§Ã£o BigQuery
  10. NotificaÃ§Ã£o de sucesso

### ðŸ“ˆ `dgu_monitoring`
**Monitoramento e qualidade de dados**

- **Agendamento**: Diariamente Ã s 10:00
- **DuraÃ§Ã£o**: ~5-10 minutos
- **Tasks**:
  1. VerificaÃ§Ã£o de existÃªncia de tabelas
  2. VerificaÃ§Ã£o de dados nas tabelas
  3. VerificaÃ§Ã£o de atualizaÃ§Ã£o dos dados
  4. VerificaÃ§Ã£o de qualidade dos dados
  5. GeraÃ§Ã£o de relatÃ³rio diÃ¡rio

## ðŸ› ï¸ Comandos Ãšteis

### Gerenciamento do Airflow

```bash
# Inicializar Airflow
./docker-scripts.sh init-airflow

# Executar via Airflow
./docker-scripts.sh run-airflow

# Ver logs do Airflow
./docker-scripts.sh logs-airflow

# Parar Airflow
./docker-scripts.sh stop-airflow

# Ver status dos containers
./docker-scripts.sh status
```

### Comandos Docker Compose

```bash
# Iniciar todos os serviÃ§os
docker-compose -f docker-compose-integrated.yml up -d

# Ver logs especÃ­ficos
docker-compose -f docker-compose-integrated.yml logs -f airflow-scheduler

# Reiniciar serviÃ§o especÃ­fico
docker-compose -f docker-compose-integrated.yml restart airflow-webserver

# Parar todos os serviÃ§os
docker-compose -f docker-compose-integrated.yml down
```

### Comandos CLI do Airflow

```bash
# Acessar CLI do Airflow
docker-compose -f docker-compose-integrated.yml exec airflow-webserver bash

# Listar DAGs
airflow dags list

# Executar DAG manualmente
airflow dags trigger dgu_futebol_pipeline

# Ver status de execuÃ§Ã£o
airflow dags state dgu_futebol_pipeline 2024-01-01

# Listar tasks de uma DAG
airflow tasks list dgu_futebol_pipeline
```

## ðŸ“ Estrutura de Arquivos

```
projeto/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ dgu_futebol_pipeline.py    # DAG principal
â”‚   â”‚   â””â”€â”€ dgu_monitoring.py          # DAG de monitoramento
â”‚   â”œâ”€â”€ plugins/
â”‚   â”‚   â”œâ”€â”€ dgu_operators.py           # Operadores customizados
â”‚   â”‚   â””â”€â”€ dgu_alerts.py              # Sistema de alertas
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ airflow.cfg                # ConfiguraÃ§Ã£o do Airflow
â”‚   â”œâ”€â”€ logs/                          # Logs do Airflow
â”‚   â”œâ”€â”€ setup_airflow_connections.py   # Script de configuraÃ§Ã£o
â”‚   â””â”€â”€ init_airflow.sh               # Script de inicializaÃ§Ã£o
â”œâ”€â”€ docker-compose-airflow.yml         # Compose apenas Airflow
â”œâ”€â”€ docker-compose-integrated.yml      # Compose integrado
â””â”€â”€ Dockerfile.airflow                 # Imagem customizada
```

## âš™ï¸ ConfiguraÃ§Ãµes AvanÃ§adas

### VariÃ¡veis do Airflow

As seguintes variÃ¡veis sÃ£o configuradas automaticamente:

```python
# ConfiguraÃ§Ãµes do projeto
dgu_project_id = 'dataglowup-458411'
dgu_dataset_main = 'DataGlowUp'
dgu_dataset_staging = 'DataGlowUp_staging'
dgu_dataset_mart = 'DataGlowUp_mart'

# ConfiguraÃ§Ãµes de execuÃ§Ã£o
dgu_max_retries = '3'
dgu_timeout_minutes = '120'
dgu_email_alerts = 'admin@dgu.com'

# ConfiguraÃ§Ãµes de monitoramento
dgu_data_freshness_hours = '96'  # 4 dias
dgu_min_players_per_team = '15'
dgu_max_players_per_team = '50'
```

### ConexÃµes

- **`google_cloud_default`**: ConexÃ£o com BigQuery
- Configurada automaticamente com as credenciais fornecidas

### Pools de Recursos

- **`dgu_extraction_pool`**: 2 slots para extraÃ§Ã£o
- **`dgu_bigquery_pool`**: 3 slots para BigQuery
- **`dgu_dbt_pool`**: 1 slot para dbt

## ðŸ”” Sistema de Alertas

### Canais Suportados

1. **Log** (padrÃ£o): Alertas nos logs do Airflow
2. **Slack**: Configure `dgu_slack_webhook`
3. **Teams**: Configure `dgu_teams_webhook`
4. **Email**: Configure `dgu_email_alerts`

### Configurar Slack

```bash
# Via interface web do Airflow
Admin > Variables > Create
Key: dgu_slack_webhook
Value: https://hooks.slack.com/services/...
```

### Configurar Teams

```bash
# Via interface web do Airflow
Admin > Variables > Create
Key: dgu_teams_webhook
Value: https://outlook.office.com/webhook/...
```

## ðŸ“Š Monitoramento

### Interface Web

- **URL**: http://localhost:8080
- **DAGs**: Visualizar status e histÃ³rico
- **Logs**: Logs detalhados de cada task
- **Gantt**: Timeline de execuÃ§Ã£o
- **Graph**: DependÃªncias entre tasks

### MÃ©tricas Importantes

- **Taxa de sucesso**: % de execuÃ§Ãµes bem-sucedidas
- **DuraÃ§Ã£o mÃ©dia**: Tempo mÃ©dio de execuÃ§Ã£o
- **Ãšltima execuÃ§Ã£o**: Timestamp da Ãºltima execuÃ§Ã£o
- **PrÃ³xima execuÃ§Ã£o**: PrÃ³ximo agendamento

## ðŸ› SoluÃ§Ã£o de Problemas

### Airflow nÃ£o inicia

```bash
# Verificar logs
docker-compose -f docker-compose-integrated.yml logs airflow-init

# Verificar permissÃµes
export AIRFLOW_UID=$(id -u)
sudo chown -R $AIRFLOW_UID:0 airflow/

# Reconstruir imagem
docker-compose -f docker-compose-integrated.yml build --no-cache
```

### DAG nÃ£o aparece

```bash
# Verificar sintaxe Python
python airflow/dags/dgu_futebol_pipeline.py

# Verificar logs do scheduler
docker-compose -f docker-compose-integrated.yml logs airflow-scheduler

# ForÃ§ar refresh
docker-compose -f docker-compose-integrated.yml exec airflow-webserver airflow dags reserialize
```

### Erro de conexÃ£o BigQuery

```bash
# Verificar credenciais
ls -la credentials/bigquery-credentials.json

# Testar conexÃ£o
docker-compose -f docker-compose-integrated.yml exec airflow-webserver python -c "
from google.cloud import bigquery
client = bigquery.Client()
print('ConexÃ£o OK!')
"

# Reconfigurar conexÃ£o
docker-compose -f docker-compose-integrated.yml exec airflow-webserver python /opt/airflow/project/airflow/setup_airflow_connections.py
```

### Task falha constantemente

```bash
# Ver logs detalhados da task
# Na interface web: DAGs > dgu_futebol_pipeline > Graph > [Task] > Logs

# Executar task manualmente
docker-compose -f docker-compose-integrated.yml exec airflow-webserver airflow tasks test dgu_futebol_pipeline extrair_estatisticas 2024-01-01

# Verificar recursos
docker stats
```

## ðŸ”„ Backup e RecuperaÃ§Ã£o

### Backup do Banco de Dados

```bash
# Backup do PostgreSQL
docker-compose -f docker-compose-integrated.yml exec postgres pg_dump -U airflow airflow > backup_airflow.sql

# Restaurar backup
docker-compose -f docker-compose-integrated.yml exec -T postgres psql -U airflow airflow < backup_airflow.sql
```

### Backup de ConfiguraÃ§Ãµes

```bash
# Backup de variÃ¡veis e conexÃµes
docker-compose -f docker-compose-integrated.yml exec airflow-webserver airflow variables export variables_backup.json
docker-compose -f docker-compose-integrated.yml exec airflow-webserver airflow connections export connections_backup.json
```

## ðŸ“ˆ PrÃ³ximos Passos

1. **Configurar alertas** via Slack/Teams
2. **Implementar testes** adicionais de qualidade
3. **Adicionar mais times** ao pipeline
4. **Configurar backup** automÃ¡tico
5. **Implementar CI/CD** para atualizaÃ§Ãµes

## ðŸŽ¯ Resumo dos BenefÃ­cios

### âœ… AutomaÃ§Ã£o Completa
- ExecuÃ§Ã£o automÃ¡tica nas terÃ§as e sextas Ã s 09:00
- Monitoramento diÃ¡rio Ã s 10:00
- Retry automÃ¡tico em caso de falhas

### âœ… Visibilidade Total
- Interface web intuitiva
- Logs detalhados de cada etapa
- MÃ©tricas de performance

### âœ… Confiabilidade
- VerificaÃ§Ãµes de qualidade automÃ¡ticas
- Alertas em tempo real
- Backup e recuperaÃ§Ã£o

### âœ… Escalabilidade
- FÃ¡cil adiÃ§Ã£o de novos times
- ConfiguraÃ§Ã£o flexÃ­vel
- Recursos otimizados

---

**ðŸŽ¯ Pipeline de dados de futebol totalmente automatizado com Apache Airflow 3.0!**

Para documentaÃ§Ã£o completa do Docker, veja: [README-Docker.md](README-Docker.md)

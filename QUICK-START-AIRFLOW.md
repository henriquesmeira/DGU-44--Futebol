# âš¡ InÃ­cio RÃ¡pido - Apache Airflow 3.0

Guia de 10 minutos para executar o pipeline DGU com orquestraÃ§Ã£o automÃ¡tica via Airflow.

## ğŸš€ Setup em 4 Passos

### 1. Preparar Credenciais (2 min)

```bash
# Criar pasta para credenciais
mkdir -p credentials

# Copiar suas credenciais do BigQuery
cp /caminho/para/suas/credenciais.json credentials/bigquery-credentials.json
```

### 2. Configurar dbt (1 min)

```bash
# Copiar template
cp DGU/profiles.yml.example DGU/profiles.yml

# Editar com seus dados (substitua os valores)
sed -i 's/seu-project-id/SEU_PROJECT_ID_REAL/g' DGU/profiles.yml
sed -i 's|/caminho/para/seu/arquivo-credenciais.json|/opt/airflow/credentials/bigquery-credentials.json|g' DGU/profiles.yml
```

### 3. Inicializar Airflow (5 min)

```bash
# Executar script de inicializaÃ§Ã£o
./docker-scripts.sh init-airflow

# Aguardar inicializaÃ§Ã£o (serÃ¡ exibido progresso)
```

### 4. Acessar Interface Web (2 min)

```bash
# Abrir no navegador
http://localhost:8080

# Fazer login
UsuÃ¡rio: airflow
Senha: airflow

# Ativar DAGs
1. Clique no toggle das DAGs:
   - dgu_futebol_pipeline âœ…
   - dgu_monitoring âœ…
```

## ğŸ¯ ExecuÃ§Ã£o AutomÃ¡tica

### ğŸ“… Agendamento Configurado

- **Pipeline Principal**: TerÃ§as e sextas Ã s 09:00
- **Monitoramento**: Diariamente Ã s 10:00

### â–¶ï¸ ExecuÃ§Ã£o Manual (Teste)

```bash
# Na interface web do Airflow:
1. Acesse "DAGs"
2. Clique em "dgu_futebol_pipeline"
3. Clique no botÃ£o "Trigger DAG" (â–¶ï¸)
4. Acompanhe o progresso na aba "Graph"
```

## ğŸ“Š Monitoramento

### Interface Web - Principais SeÃ§Ãµes

- **DAGs**: Lista e status das DAGs
- **Graph**: VisualizaÃ§Ã£o das dependÃªncias
- **Gantt**: Timeline de execuÃ§Ã£o
- **Logs**: Logs detalhados de cada task

### ğŸ“ˆ MÃ©tricas Importantes

```bash
# Verificar Ãºltima execuÃ§Ã£o
DAGs > dgu_futebol_pipeline > Last Run

# Ver logs de uma task especÃ­fica
Graph > [Nome da Task] > Logs

# HistÃ³rico de execuÃ§Ãµes
Browse > Task Instances
```

## ğŸ”§ Comandos Ãšteis

### Gerenciamento RÃ¡pido

```bash
# Ver status dos serviÃ§os
./docker-scripts.sh status

# Ver logs em tempo real
./docker-scripts.sh logs-airflow

# Parar Airflow
./docker-scripts.sh stop-airflow

# Reiniciar Airflow
./docker-scripts.sh stop-airflow
./docker-scripts.sh run-airflow
```

### Comandos AvanÃ§ados

```bash
# Executar DAG via CLI
docker-compose -f docker-compose-integrated.yml exec airflow-webserver \
  airflow dags trigger dgu_futebol_pipeline

# Listar todas as DAGs
docker-compose -f docker-compose-integrated.yml exec airflow-webserver \
  airflow dags list

# Ver status de uma execuÃ§Ã£o
docker-compose -f docker-compose-integrated.yml exec airflow-webserver \
  airflow dags state dgu_futebol_pipeline 2024-01-01
```

## âœ… Verificar Resultados

### 1. No Airflow (Interface Web)

```bash
# Verificar se pipeline executou com sucesso
DAGs > dgu_futebol_pipeline > Graph
# Todas as tasks devem estar verdes âœ…
```

### 2. No BigQuery Console

```bash
# Acessar: https://console.cloud.google.com/bigquery

# Verificar datasets criados:
1. DataGlowUp (dados brutos) - 6 tabelas
2. DataGlowUp_staging (views) - 6 views  
3. DataGlowUp_mart (finais) - 3 tabelas
```

### 3. Logs do Sistema

```bash
# Ver logs da Ãºltima execuÃ§Ã£o
./docker-scripts.sh logs-airflow | grep "dgu_futebol_pipeline"

# Verificar se nÃ£o hÃ¡ erros
./docker-scripts.sh logs-airflow | grep -i error
```

## ğŸ› SoluÃ§Ã£o RÃ¡pida de Problemas

### Airflow nÃ£o carrega

```bash
# Verificar se Docker estÃ¡ rodando
docker ps

# Verificar logs de inicializaÃ§Ã£o
docker-compose -f docker-compose-integrated.yml logs airflow-init

# Reconstruir se necessÃ¡rio
docker-compose -f docker-compose-integrated.yml build --no-cache
```

### DAG nÃ£o aparece

```bash
# Verificar sintaxe do arquivo Python
python airflow/dags/dgu_futebol_pipeline.py

# ForÃ§ar refresh das DAGs
# Na interface web: Admin > Configuration > Refresh
```

### Erro de credenciais

```bash
# Verificar se arquivo existe
ls -la credentials/bigquery-credentials.json

# Verificar permissÃµes
chmod 600 credentials/bigquery-credentials.json

# Testar conexÃ£o
docker-compose -f docker-compose-integrated.yml exec airflow-webserver python -c "
from google.cloud import bigquery
client = bigquery.Client()
print('âœ… ConexÃ£o OK!')
"
```

### Pipeline falha

```bash
# Ver logs especÃ­ficos da task que falhou
# Interface web: Graph > [Task Vermelha] > Logs

# Executar task manualmente para debug
docker-compose -f docker-compose-integrated.yml exec airflow-webserver \
  airflow tasks test dgu_futebol_pipeline extrair_estatisticas 2024-01-01
```

## ğŸ‰ PrÃ³ximos Passos

### 1. Configurar Alertas

```bash
# Slack (opcional)
Admin > Variables > Create
Key: dgu_slack_webhook
Value: https://hooks.slack.com/services/...

# Email (opcional)
Admin > Variables > Create
Key: dgu_email_alerts
Value: seu-email@exemplo.com
```

### 2. Personalizar Agendamento

```bash
# Editar arquivo da DAG
nano airflow/dags/dgu_futebol_pipeline.py

# Alterar linha:
schedule_interval='0 9 * * 2,5'  # TerÃ§as e sextas Ã s 09:00

# Para executar diariamente Ã s 08:00:
schedule_interval='0 8 * * *'
```

### 3. Adicionar Mais Times

```bash
# Editar extract.py para incluir novos times
# Atualizar dbt_project.yml com configuraÃ§Ãµes dos novos seeds
# Recriar modelos dbt conforme necessÃ¡rio
```

## ğŸ“š DocumentaÃ§Ã£o Completa

- **Airflow**: [README-AIRFLOW.md](README-AIRFLOW.md)
- **Docker**: [README-Docker.md](README-Docker.md)
- **Projeto**: [PROJETO_COMPLETO.md](PROJETO_COMPLETO.md)

---

**âš¡ Em 10 minutos vocÃª tem um pipeline de dados de futebol totalmente automatizado!**

**ğŸ• ExecuÃ§Ã£o automÃ¡tica**: TerÃ§as e sextas Ã s 09:00  
**ğŸ“Š Monitoramento**: http://localhost:8080  
**ğŸ¯ Dados atualizados**: BigQuery Console
